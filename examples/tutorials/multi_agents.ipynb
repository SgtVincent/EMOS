{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a521496-685a-4073-bbc5-bdbc13642aa1",
   "metadata": {},
   "source": [
    "# Interacting with Articulated Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a60d63-feeb-4b05-aec2-b9196117c9d1",
   "metadata": {},
   "source": [
    "In this tutorial we will show how to interact in Habitat via articulated agents. These are agents composed of different parts which can be articulated. Examples of these agents include different commercial robots (such as Spot, Fetch, Franka) or humanoids.\n",
    "In this tutorial we will explore how to interact in Habitat with such agents. We will cover the following topics:\n",
    "\n",
    "- How to initialize an agent\n",
    "- Moving an agent around the scene\n",
    "- Dynamic vs Kinematic Simulation\n",
    "- Interacting with objects\n",
    "- Interacting with Actions\n",
    "- Multi-Agent simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bf54d-c588-4401-a7e6-0525f480fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import habitat_sim\n",
    "import magnum as mn\n",
    "import warnings\n",
    "from habitat.tasks.rearrange.rearrange_sim import RearrangeSim\n",
    "warnings.filterwarnings('ignore')\n",
    "from habitat_sim.utils.settings import make_cfg\n",
    "from matplotlib import pyplot as plt\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "from habitat.articulated_agents.robots import FetchRobot\n",
    "from habitat.config.default import get_agent_config\n",
    "from habitat.config.default_structured_configs import ThirdRGBSensorConfig, HeadRGBSensorConfig, HeadPanopticSensorConfig\n",
    "from habitat.config.default_structured_configs import SimulatorConfig, HabitatSimV0Config, AgentConfig\n",
    "from habitat.config.default import get_agent_config\n",
    "import habitat\n",
    "from habitat_sim.physics import JointMotorSettings, MotionType\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import git, os\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "os.chdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430d6c2-16ab-4754-944e-7c1637d1859c",
   "metadata": {},
   "source": [
    "# Initializing a scene with agents\n",
    "The first thing we want to do is to initialize the simulator to include different agents. \n",
    "\n",
    "In the first part of this tutorial we will use `RearrangeSim` as our simulator, which is an abstraction over [HabitatSimulator](https://aihabitat.org/docs/habitat-lab/habitat.core.simulator.Simulator.html) and includes functionalities to update agent cameras and position or interact with objects. In the second part of the tutorial, we will be defining agent actions and will be using a `RearrangeEnvironment`, which contains a reference to the simulator, as well as functions to define and execute agent actions, obtain rewards or termination conditions. The RearrangeEnvironment will also be used to train agents via RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334efd7-514a-4ce6-a958-d1b31a537a71",
   "metadata": {},
   "source": [
    "## Defining agent configurations\n",
    "We start by defining a configuration for each agent we want to add. Articulated agents are represented as any other articulated object, and are therefore defined via an URDF file. While this file is enough to represent the agent as an object, it doesn't include a way to easily set its base position, reset its joints, move a specific part or query other attributes.\n",
    "\n",
    "To simplify this, we provide an abstraction, `ArticulatedAgent`, which will wrap habitat-sim's ManagedArticulatedObject class initialized from the URDF and provide functionalities that are commonly useful for agent control. You can view the different ArticulatedAgents (robots and humanoids) [here](https://github.com/facebookresearch/habitat-lab/tree/main/habitat-lab/habitat/articulated_agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec3be8-02aa-4238-8875-da65d6308f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent configuration\n",
    "main_agent_config = AgentConfig()\n",
    "urdf_path = os.path.join(data_path, \"robots/hab_fetch/robots/hab_fetch.urdf\")\n",
    "main_agent_config.articulated_agent_urdf = urdf_path\n",
    "main_agent_config.articulated_agent_type = \"FetchRobot\"\n",
    "\n",
    "# Define sensors that will be attached to this agent, here a third_rgb sensor and a head_rgb.\n",
    "# We will later talk about why we are giving the sensors these names\n",
    "main_agent_config.sim_sensors = {\n",
    "    \"third_rgb\": ThirdRGBSensorConfig(),\n",
    "    \"head_rgb\": HeadRGBSensorConfig(),\n",
    "}\n",
    "\n",
    "# We create a dictionary with names of agents and their corresponding agent configuration\n",
    "agent_dict = {\"main_agent\": main_agent_config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f679d-2d42-4b7a-9431-f491d006a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim_cfg(agent_dict):\n",
    "    # Start the scene config\n",
    "    sim_cfg = SimulatorConfig(type=\"RearrangeSim-v0\")\n",
    "    \n",
    "    # This is for better graphics\n",
    "    sim_cfg.habitat_sim_v0.enable_hbao = True\n",
    "    sim_cfg.habitat_sim_v0.enable_physics = True\n",
    "\n",
    "    \n",
    "    # Set up an example scene\n",
    "    sim_cfg.scene = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/scenes/103997919_171031233.scene_instance.json\")\n",
    "    sim_cfg.scene_dataset = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/hab3-hssd.scene_dataset_config.json\")\n",
    "    sim_cfg.additional_object_paths = [os.path.join(data_path, 'objects/ycb/configs/')]\n",
    "\n",
    "    \n",
    "    cfg = OmegaConf.create(sim_cfg)\n",
    "\n",
    "    # Set the scene agents\n",
    "    cfg.agents = agent_dict\n",
    "    cfg.agents_order = list(cfg.agents.keys())\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def init_rearrange_sim(agent_dict):\n",
    "    # Start the scene config\n",
    "    sim_cfg = make_sim_cfg(agent_dict)    \n",
    "    cfg = OmegaConf.create(sim_cfg)\n",
    "    \n",
    "    # Create the scene\n",
    "    sim = RearrangeSim(cfg)\n",
    "\n",
    "    # This is needed to initialize the agents\n",
    "    sim.agents_mgr.on_new_scene()\n",
    "\n",
    "    # For this tutorial, we will also add an extra camera that will be used for third person recording.\n",
    "    camera_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    camera_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    camera_sensor_spec.uuid = \"scene_camera_rgb\"\n",
    "\n",
    "    # TODO: this is a bit dirty but I think its nice as it shows how to modify a camera sensor...\n",
    "    sim.add_sensor(camera_sensor_spec, 0)\n",
    "\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a1166-4943-419f-9a4d-2c560ffe6bb6",
   "metadata": {},
   "source": [
    "## Initializing the scene\n",
    "We can now initialize the scene. As mentioned before, we will be using here `RearrangeSim` to easily be able to interact with objects.\n",
    "\n",
    "We create a scene init function that will take as input a dictionary of agent configurations, as the one we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a4c4-b4c4-4b52-9212-74564e9d2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = init_rearrange_sim(agent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2edf9-9e1c-48ba-b285-e83962d55d70",
   "metadata": {},
   "source": [
    "We just initialized our scene! We can now query and set our agent position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a769a1-a944-4bb8-8860-5ea8de5f7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pos = mn.Vector3(-5.5,0,-1.5)\n",
    "art_agent = sim.articulated_agent\n",
    "# We will see later about this\n",
    "art_agent.sim_obj.motion_type = MotionType.KINEMATIC\n",
    "print(\"Current agent position:\", art_agent.base_pos)\n",
    "art_agent.base_pos = init_pos \n",
    "print(\"New agent position:\", art_agent.base_pos)\n",
    "# We take a step to update agent position\n",
    "_ = sim.step({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fc09d-4a79-4697-9feb-c7210879601d",
   "metadata": {},
   "source": [
    "We can also take observations in the environment. Here we get three sensors, two of which we defined in the config and one which we added afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949822e-410b-41bb-89ea-b898fc9a6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = sim.get_sensor_observations()\n",
    "print(observations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3565f-1c3a-4880-8877-8a3af9893d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,len(observations.keys()))\n",
    "\n",
    "for ind, name in enumerate(observations.keys()):\n",
    "    ax[ind].imshow(observations[name])\n",
    "    ax[ind].set_axis_off()\n",
    "    ax[ind].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152136f-7a43-410d-8a41-738e40d46116",
   "metadata": {},
   "source": [
    "We will now pick the object. In this example, we will directly attach the object to the robot arm, without animating the arm in any way. We also provide a way to train policies so that the arm approaches the object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd3c0b-5254-4a7b-af60-9c69a1dc7624",
   "metadata": {},
   "source": [
    "# Defining agent actions\n",
    "So far, we have been controlling agents by directly updating the robot parameters. In  many cases, you may want to abstract interaction into actions that update the robot. These actions can then be called by a planner or a learned policy. In this section we will show how to define and control agents with these actions. The Habitat Quickstart provides more instructions into how to add actions https://aihabitat.org/docs/habitat-lab/quickstart.html.\n",
    "TODO: point to skills tutorial\n",
    "\n",
    "To execute actions, we will be using the `Env`, which is an object that contains a simulator instance as well as a set of action definitions and specifiable rewards. We will not be going through the specifiable rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2d066-871e-4c11-ac7d-751d3e10ca2d",
   "metadata": {},
   "source": [
    "## Defining an environment\n",
    "We will start by defining the environment class. A key difference is that now we also define actions that the environment will have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b75f5-f252-4655-997a-ed5791d4265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from habitat.config.default_structured_configs import TaskConfig, EnvironmentConfig, DatasetConfig, HabitatConfig\n",
    "from habitat.config.default_structured_configs import ArmActionConfig, BaseVelocityActionConfig, OracleNavActionConfig, ActionConfig\n",
    "from habitat.core.env import Env\n",
    "def make_sim_cfg(agent_dict):\n",
    "    # Start the scene config\n",
    "    sim_cfg = SimulatorConfig(type=\"RearrangeSim-v0\")\n",
    "    \n",
    "    # Enable Horizon Based Ambient Occlusion (HBAO) to approximate shadows.\n",
    "    sim_cfg.habitat_sim_v0.enable_hbao = True\n",
    "    \n",
    "    sim_cfg.habitat_sim_v0.enable_physics = True\n",
    "\n",
    "    \n",
    "    # Set up an example scene\n",
    "    sim_cfg.scene = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/scenes/103997919_171031233.scene_instance.json\")\n",
    "    sim_cfg.scene_dataset = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/hab3-hssd.scene_dataset_config.json\")\n",
    "    sim_cfg.additional_object_paths = [os.path.join(data_path, 'objects/ycb/configs/')]\n",
    "\n",
    "    \n",
    "    cfg = OmegaConf.create(sim_cfg)\n",
    "\n",
    "    # Set the scene agents\n",
    "    cfg.agents = agent_dict\n",
    "    cfg.agents_order = list(cfg.agents.keys())\n",
    "    return cfg\n",
    "\n",
    "def make_hab_cfg(agent_dict, action_dict):\n",
    "    sim_cfg = make_sim_cfg(agent_dict)\n",
    "    task_cfg = TaskConfig(type=\"RearrangeEmptyTask-v0\")\n",
    "    task_cfg.actions = action_dict\n",
    "    env_cfg = EnvironmentConfig()\n",
    "    dataset_cfg = DatasetConfig(type=\"RearrangeDataset-v0\", data_path=\"data/hab3_bench_assets/episode_datasets/small_large.json.gz\")\n",
    "    \n",
    "    \n",
    "    hab_cfg = HabitatConfig()\n",
    "    hab_cfg.environment = env_cfg\n",
    "    hab_cfg.task = task_cfg\n",
    "    hab_cfg.dataset = dataset_cfg\n",
    "    hab_cfg.simulator = sim_cfg\n",
    "    hab_cfg.simulator.seed = hab_cfg.seed\n",
    "\n",
    "    return hab_cfg\n",
    "\n",
    "def init_rearrange_env(agent_dict, action_dict):\n",
    "    hab_cfg = make_hab_cfg(agent_dict, action_dict)\n",
    "    res_cfg = OmegaConf.create(hab_cfg)\n",
    "    return Env(res_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f76ba-51bc-4311-9bb1-dce37a6fa3a9",
   "metadata": {},
   "source": [
    "# Multi-Agent Interaction\n",
    "So far, we've been executing actions with a single agent. Habitat allows multi-agent execution, we will be looking here at how to do it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180c738-eae3-41d6-8dc6-04860710bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will download spot to show interaction between the spot robot and fetch\n",
    "# ! python -m habitat_sim.utils.datasets_download --uids hab_spot_arm --no-replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1c1f2-4e4a-4a57-8242-d2978d6982fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference is in how we define the agent_dict.\n",
    "# Important: When using more than one agent, we should call them agent_{idx} with idx being between 0 and\n",
    "# the number of agents. This is required so that we can parse actions\n",
    "import copy\n",
    "second_agent_config = copy.deepcopy(main_agent_config)\n",
    "second_agent_config.articulated_agent_urdf = os.path.join(data_path, \"robots/hab_spot_arm/urdf/hab_spot_arm.urdf\")\n",
    "second_agent_config.articulated_agent_type = \"SpotRobot\"\n",
    "\n",
    "\n",
    "agent_dict = {\"agent_0\": main_agent_config, \"agent_1\": second_agent_config}\n",
    "multi_agent_action_dict = {\n",
    "    \"agent_0_oracle_magic_grasp_action\": ArmActionConfig(\n",
    "        type=\"MagicGraspAction\"\n",
    "    ),\n",
    "    \"agent_0_base_velocity_action\": BaseVelocityActionConfig(),\n",
    "    \"agent_0_oracle_coord_action\": OracleNavActionConfig(\n",
    "        type=\"OracleNavCoordinateAction\",\n",
    "        spawn_max_dist_to_obj=2.0,\n",
    "        motion_control=\"base_velocity\",\n",
    "    ),\n",
    "    \"agent_1_oracle_magic_grasp_action\": ArmActionConfig(\n",
    "        type=\"MagicGraspAction\"\n",
    "    ),\n",
    "    \"agent_1_base_velocity_action\": BaseVelocityActionConfig(),\n",
    "    \"agent_1_oracle_coord_action\": OracleNavActionConfig(\n",
    "        type=\"OracleNavCoordinateAction\",\n",
    "        spawn_max_dist_to_obj=2.0,\n",
    "        motion_control=\"base_velocity_non_cylinder\",\n",
    "        navmesh_offset=[[0.0, 0.0], [0.25, 0.0], [-0.25, 0.0]],\n",
    "    ),\n",
    "}\n",
    "\n",
    "env = init_rearrange_env(agent_dict, multi_agent_action_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387461f-cc92-4edc-83ec-89860e2db173",
   "metadata": {},
   "source": [
    "#### The environment takes care about adding prefixes to observations, so that you can query the observation of each of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fd9aa-84dc-4d21-9099-17c570a025c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()\n",
    "_, ax = plt.subplots(1,len(observations.keys()))\n",
    "\n",
    "for ind, name in enumerate(observations.keys()):\n",
    "    ax[ind].imshow(observations[name])\n",
    "    ax[ind].set_axis_off()\n",
    "    ax[ind].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574a80f-ea35-4ace-8af7-d79ab7aa2c4a",
   "metadata": {},
   "source": [
    "As before, we can call actions on the agents, prepending the agent_name before the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57d73d-4117-4918-aa8d-f2599ae5a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rom = env.sim.get_rigid_object_manager()\n",
    "# env.sim.articulated_agent.base_pos = init_pos\n",
    "# As before, we get a navigation point next to an object id\n",
    "\n",
    "obj_id = env.sim.scene_obj_ids[0]\n",
    "first_object = rom.get_object_by_id(obj_id)\n",
    "\n",
    "object_trans = first_object.translation\n",
    "observations = []\n",
    "\n",
    "# Walk towards the object\n",
    "\n",
    "agent_displ = np.inf\n",
    "agent_rot = np.inf\n",
    "while agent_displ > 1e-9 or agent_rot > 1e-9:\n",
    "    prev_rot_0 = env.sim.agents_mgr[0].articulated_agent.base_rot\n",
    "    prev_pos_0 = env.sim.agents_mgr[0].articulated_agent.base_pos\n",
    "    prev_rot_1 = env.sim.agents_mgr[1].articulated_agent.base_rot\n",
    "    prev_pos_1 = env.sim.agents_mgr[1].articulated_agent.base_pos\n",
    "    action_dict = {\n",
    "        \"action\": (\"agent_0_oracle_coord_action\", \"agent_1_oracle_coord_action\"), \n",
    "        \"action_args\": {\n",
    "              \"agent_0_oracle_nav_lookat_action\": object_trans,\n",
    "              \"agent_0_mode\": 1,\n",
    "              \"agent_1_oracle_nav_lookat_action\": object_trans,\n",
    "              \"agent_1_mode\": 1\n",
    "          }\n",
    "    }\n",
    "    observations.append(env.step(action_dict))\n",
    "    \n",
    "    cur_rot_0 = env.sim.agents_mgr[0].articulated_agent.base_rot\n",
    "    cur_pos_0 = env.sim.agents_mgr[0].articulated_agent.base_pos\n",
    "    cur_rot_1 = env.sim.agents_mgr[1].articulated_agent.base_rot\n",
    "    cur_pos_1 = env.sim.agents_mgr[1].articulated_agent.base_pos\n",
    "    agent_displ_0 = (cur_pos_0 - prev_pos_0).length()\n",
    "    agent_rot_0 = np.abs(cur_rot_0 - prev_rot_0)\n",
    "    agent_displ_1 = (cur_pos_1 - prev_pos_1).length()\n",
    "    agent_rot_1 = np.abs(cur_rot_1 - prev_rot_1)\n",
    "    agent_displ = max(agent_displ_0, agent_displ_1)\n",
    "    agent_rot = max(agent_rot_0, agent_rot_1)\n",
    "    # print(agent_rot, agent_displ)\n",
    "vut.make_video(\n",
    "    observations,\n",
    "    \"agent_1_third_rgb\",\n",
    "    \"color\",\n",
    "    \"robot_tutorial_video\",\n",
    "    open_vid=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
